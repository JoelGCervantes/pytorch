{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4ff3e0",
   "metadata": {},
   "source": [
    "# Pytorch Tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fd616",
   "metadata": {},
   "source": [
    "In-depth intro to ```torch.Tensor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119575dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef81bc",
   "metadata": {},
   "source": [
    "# Creating tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d28b3",
   "metadata": {},
   "source": [
    "simplest way to create a tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3ee682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3,4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc020d",
   "metadata": {},
   "source": [
    "- This tensor is 2-dimensional with 3 row and 4 colums.\n",
    "- ```torch.Tensor``` is an aliance for ```torch.FloatTensor```. PyTorch tensors are populated with 32-bit floating point nums by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bad08c",
   "metadata": {},
   "source": [
    "**Terminology**: \n",
    "\n",
    "- 1-dimensional tensor = vector \n",
    "\n",
    "- 2-dimensional vector = *matrix*\n",
    "\n",
    "- anything with more than two dimensions = tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333b88e",
   "metadata": {},
   "source": [
    "initializing tensors with zeros, ones, random vals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd35d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2,3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d621922",
   "metadata": {},
   "source": [
    "# Random Tensors & Seeding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1011d",
   "metadata": {},
   "source": [
    "For reproducibility reasons we can seed the random number generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57403803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2,3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c6e5",
   "metadata": {},
   "source": [
    "# Tensor Shapes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58076191",
   "metadata": {},
   "source": [
    "Creating two tensors with the same *shape* - same number of dimensions and same number of cells in each dimension. ```torch.*_like()`` methods achieve this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90680148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,2,3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52bc070",
   "metadata": {},
   "source": [
    "# Tensor Data Types "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b60669",
   "metadata": {},
   "source": [
    "Setting the datatype of a tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179307e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2,3), dtype=torch.float64) * 20\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d08ede",
   "metadata": {},
   "source": [
    "# Math & Logic with PytTorch Tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5df304",
   "metadata": {},
   "source": [
    "basic arithmetic with PyTorch tensors. How tensors interact with simple scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747dc388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2,2) + 1\n",
    "twos = torch.ones(2,2) * 2\n",
    "threes = (torch.ones(2,2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1181936",
   "metadata": {},
   "source": [
    "More arithmetic done with tensor math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8ec6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours \n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours \n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ed256",
   "metadata": {},
   "source": [
    "# Tensor Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022fc34",
   "metadata": {},
   "source": [
    "In general tensors of differnt shape cannot perform binary operations on each other. The exception to this is Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f946c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6146, 0.5999, 0.5013, 0.9397],\n",
      "        [0.8656, 0.5207, 0.6865, 0.3614]])\n",
      "tensor([[1.2291, 1.1998, 1.0026, 1.8793],\n",
      "        [1.7312, 1.0413, 1.3730, 0.7228]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2,4)\n",
    "doubled = rand * (torch.ones(1,4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38da4f",
   "metadata": {},
   "source": [
    "the (1 , 4) tensor is being multiplied by *both* rows of the (2, 4) tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17cea2",
   "metadata": {},
   "source": [
    "This situation shows up in Deep Learning when we need to multiply a tensor of learning weights by a *batch* of input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5097a",
   "metadata": {},
   "source": [
    "rules for broadcasting: \n",
    "- Each tensor must have atleast one dimension - no empty tensors \n",
    "- Comparing the dimension sizes of the two tensors, *going from last to first*:\n",
    "- - Each dimension must be equal, *or*\n",
    "- - one of the dimensions must be size 1, *or*\n",
    "- - The dimension does not exist in one of the tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f4855",
   "metadata": {},
   "source": [
    "Examples of broadcasting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f3b6bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]]])\n",
      "tensor([[[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]]])\n",
      "tensor([[[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(  3, 2) # 3rd & 2nd dims are identical to a, dim 1 is abscent \n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(  3, 1) # 3rd dim = 1, 2nd dim identical ro a \n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(  1, 2) # 3rd dim is identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65295d11",
   "metadata": {},
   "source": [
    "# More Math with Tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe5a09",
   "metadata": {},
   "source": [
    "major categories of tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7ad18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common functions: \n",
      "tensor([[0.2648, 0.8928, 0.9773, 0.0365],\n",
      "        [0.9614, 0.3090, 0.1713, 0.8608]])\n",
      "tensor([[1., 1., -0., 1.],\n",
      "        [1., 1., -0., -0.]])\n",
      "tensor([[ 0.,  0., -1.,  0.],\n",
      "        [ 0.,  0., -1., -1.]])\n",
      "tensor([[ 0.2648,  0.5000, -0.5000,  0.0365],\n",
      "        [ 0.5000,  0.3090, -0.1713, -0.5000]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise, equality comparisons:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.4648, 0.4491],\n",
      "        [0.6265, 0.9411]])\n",
      "tensor([[1.3944, 1.3473],\n",
      "        [1.8796, 2.8234]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.4918, -0.8707],\n",
      "        [-0.8707,  0.4918]]),\n",
      "S=tensor([3.8902, 0.3610]),\n",
      "V=tensor([[-0.5970, -0.8023],\n",
      "        [-0.8023,  0.5970]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/mb0trvvd6wl49399bdq8hg880000gn/T/ipykernel_66320/792472865.py:46: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Cross.cpp:67.)\n",
      "  print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n"
     ]
    }
   ],
   "source": [
    "# commons functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print(\"common functions: \")\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trig functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise, equality comparisons:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2) \n",
    "print(torch.eq(d, e)) # returns tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d)) # returns single element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor \n",
    "print(torch.mean(d)) # average\n",
    "print(torch.std(d)) # stdandard deviation \n",
    "print(torch.prod(d)) # product of all numbers \n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear alg operations \n",
    "v1 = torch.tensor([1., 0., 0.])     # x unit vector \n",
    "v2 = torch.tensor([0., 1., 0.])     # y unit vector\n",
    "m1 = torch.rand(2, 2)               # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three tmes identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                          # 3 times m1\n",
    "print(torch.svd(m3))               # singular value decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946537c",
   "metadata": {},
   "source": [
    "In place modification of tensors. Good for optimization and freeing up memory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feced623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b: tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print(f'a: {a}')\n",
    "print(torch.sin(a))         # this operation is a new tensor in memory \n",
    "print(a)                    # a has not changed \n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print(f'\\nb: {b}')\n",
    "print(torch.sin_(b))       # b has changed\n",
    "print(b)                   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08366e",
   "metadata": {},
   "source": [
    "arithmetic operations work similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47f249fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.7765, 0.3534],\n",
      "        [0.7016, 0.6826]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.7765, 1.3534],\n",
      "        [1.7016, 1.6826]])\n",
      "tensor([[1.7765, 1.3534],\n",
      "        [1.7016, 1.6826]])\n",
      "tensor([[0.7765, 0.3534],\n",
      "        [0.7016, 0.6826]])\n",
      "\n",
      "After Multiplying:\n",
      "tensor([[0.6030, 0.1249],\n",
      "        [0.4923, 0.4660]])\n",
      "tensor([[0.6030, 0.1249],\n",
      "        [0.4923, 0.4660]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('before')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter Multiplying:')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b365a",
   "metadata": {},
   "source": [
    "in place arithmetic functions are methods on the ```torch.Tensor``` object not attached to the ```torch``` module like many other functions (```torch.sin()```) \n",
    "\n",
    "from ```a.add_(b) the *calling* tensor is the one that gets changed in place. \n",
    "\n",
    "other option for placing result of computation in an existing allocated tensor. Many methods and functions seen so far (including creation methods) - have an ```out``` arg that lets us specify a tensor to receive the output. if the ```out``` tensor is the correct shape and ```dtype``` this can happen without a new memory allocation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510013e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original c: tensor([[0.5627, 0.1740],\n",
      "        [0.4151, 0.4759]])\n",
      "changed c: tensor([[0.5866, 0.4727],\n",
      "        [0.6570, 0.3235]])\n",
      "changed c again: tensor([[0.1544, 0.3413],\n",
      "        [0.5993, 0.6036]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.rand(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(f'original c: {c}')\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(f'changed c: {c}')        # contents of c changed\n",
    "\n",
    "assert c is d                   # test c & d are the same obj, not just contains equal values  \n",
    "assert id(c), old_id            # ensure new matrix c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c)         # works for creation \n",
    "print(f'changed c again: {c}')  # c has changed again \n",
    "assert id(c), old_id            # still the same obj "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073a67a",
   "metadata": {},
   "source": [
    "# Copying Tensors \n",
    "Like any python object, assigning a tensor to a variable makes the variable a *label* of the tensor, and does not copy it. example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbb38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change present in python variable b:\n",
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a \n",
    "\n",
    "a[0][1] = 561           # make a change to tensor a \n",
    "print(f'change present in python variable b:\\n{b}')                # b is also altered "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a34cc6",
   "metadata": {},
   "source": [
    "Separate copy of the data using ```clone()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1f5c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor variable b remains unchanged:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a           # different objects in memory\n",
    "print(torch.eq(a,b))        # b has same contents as a \n",
    "\n",
    "a[0][1] = 561               # a changes \n",
    "print(f'tensor variable b remains unchanged:\\n{b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f200e3",
   "metadata": {},
   "source": [
    "**Important note when using ```clone```** if source tensor has autograd, then the clone will to. Many cases this is what we want. If the model has multiple computation paths in its ```forward()``` method, and *both* the original tensor and its clone contribute to the model's output. To enable learning we want autograd turned on for both tensors. \n",
    "\n",
    "On other hand, if were doing computation where *neither* the original tensor or its clone need to track gradients, then as long as the source tensor has autograd turned off it is okay. \n",
    "\n",
    "*Third case* when performing a computation in models ```forward()``` function, where gradients are turned on for everything by default, but what to pull out some values mid-stream to generate metrics. In this case, we *don't* want the cloned copy to track gradients - performance improved with autograd's history tracking turned off. For this operation we can use ```.detach()``` method on source tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63018a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([[0.8553, 0.8856],\n",
      "        [0.5618, 0.9886]], requires_grad=True)\n",
      "cloned tensor:\n",
      "tensor([[0.8553, 0.8856],\n",
      "        [0.5618, 0.9886]], grad_fn=<CloneBackward0>)\n",
      "cloned tensor detached:\n",
      "tensor([[0.8553, 0.8856],\n",
      "        [0.5618, 0.9886]])\n",
      "original tensor:\n",
      "tensor([[0.8553, 0.8856],\n",
      "        [0.5618, 0.9886]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True)         # turn on autograd \n",
    "print(f'original tensor:\\n{a}')\n",
    "\n",
    "b = a.clone()\n",
    "print(f'cloned tensor:\\n{b}')\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(f'cloned tensor detached:\\n{c}')\n",
    "\n",
    "print(f'original tensor:\\n{a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78df50d",
   "metadata": {},
   "source": [
    "### explanation\n",
    "- ```a``` is created with ```requires_grad=True``` turned on. the property is present when printing ```a```. autograd and computation history is turned on. \n",
    "\n",
    "- we clone ```a``` and label it ```b```. When printing ```b``` we can see its tracking and computation history. It has inherited ```a```'s autograd settings, and added to the computation history. \n",
    "\n",
    "- ```c``` is a clone of ```a``` using ```.detach()```. when printing we see no computation history and no ```requires_grad=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
