{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb59b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # used for all pytorch things \n",
    "import torch.nn as nn # for torch.nn.Module, the parent for PyTorch models \n",
    "import torch.nn.functional as F # for activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd2ff5",
   "metadata": {},
   "source": [
    "![alt text](imgs/screenshot1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a785d",
   "metadata": {},
   "source": [
    "Above is LeNet-5, one of the earliest convolutional nns. It was built to read small images of handwritten numbers (MNIST dataset) and correctly classify which digit was represented in the image. \n",
    "\n",
    "how it works:\n",
    "- Layer C1 is a convolutional layer. It scans the input image for features it learned during training. It outputs a map of where it saw each of its learned features in the image. the \"activation map\" is downsampled in layer S2. \n",
    "\n",
    "- Layer C3 is another convolutional layer. This layer scans C1's activation map for combinations of features. It also puts out an activation map describing the spatial locations of these feature combinations which is downsampled in layer S4. \n",
    "\n",
    "- Finally the fully-connected layers at the end, F5, F6, and OUTPUT are a classifier that takes the final activation map and classifies it into one of ten bins representing 10 digits.\n",
    "\n",
    "In code the nn is represented by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e66950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution \n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) # one input channel (grayscale image), 6 output channels (feature maps), kernel size (3x3) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine ooperation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6 * 6 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Max pooling over a (2,2) window \n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "            # if the size is a square you can only specify a single number \n",
    "            x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x \n",
    "        \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:] # all dimensions except the batch dimension \n",
    "            num_features = 1\n",
    "            for s in size: \n",
    "                num_features *= s \n",
    "            return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcb1b3",
   "metadata": {},
   "source": [
    "The above code shows a typical PyTorch Model: \n",
    "- inherits from ```torch.nn.Module``` - modules may be nested, in fact, even the ```Conv2d``` and ```Linear``` classes inherit for ```torch.nn.Module``` \n",
    "\n",
    "- A model will have a ```__init__()``` funtion where it instantiates its layers, and loads any data artifacts it might need (e.g. an NLP model might load a vocabulary)\n",
    "\n",
    "- A model will have a ```forward()``` function. This is the actual computation happens. An input is passes through the network layers and various functions to generate output. \n",
    "\n",
    "- Aside from these facts we can build our model like any other Python class adding whatever properties and methods needed to support model computation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cc759",
   "metadata": {},
   "source": [
    "understanding the code \n",
    "## structure \n",
    "```__init__()``` \n",
    "gathers the tools needed to start \n",
    "\n",
    "```conv1, conv2``` \n",
    "concolutional layers \"feature detectors\" such as edges curves, corners. \n",
    "```conv1``` - takes one input channel and creates 6 \"feature maps\" using a 3x3 kernel. \n",
    "\n",
    "```self.fc*``` \n",
    "\"Fully connected\" (linear) layers. Once convolution layers have found the features these layers act as a traditional brain to make snese of them and decide: \"Based on these curves, this is likely the number 5.\"\n",
    "\n",
    "## Data Flow \n",
    "```forward()``` method defines the path the image takes through the network. \n",
    "\n",
    "step 1. <strong>Convolution</strong>: scans the image for patterns \n",
    "\n",
    "step 2. <strong>ReLU</strong>: Activation function that turns negative values to zero. (add non-linearity)\n",
    "\n",
    "step 3. <strong>Max Pooling</strong>: shrinks the image size by half to reduce computation and focus on the most important features. \n",
    "\n",
    "step 4, <strong>Flattening</strong>: Converts the 3D cube of the data into a 1D long list of numbers, so the \"linear\" layers can read it. \n",
    "\n",
    "step 5. <strong>Output</strong>: The final layer ```fc3``` produces 10 numbers. The highest number represents the networks \"guess\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe98e0",
   "metadata": {},
   "source": [
    "Lets instatiate this object and run a sample input through it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
