{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb59b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # used for all pytorch things \n",
    "import torch.nn as nn # for torch.nn.Module, the parent for PyTorch models \n",
    "import torch.nn.functional as F # for activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd2ff5",
   "metadata": {},
   "source": [
    "![alt text](imgs/screenshot1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a785d",
   "metadata": {},
   "source": [
    "Above is LeNet-5, one of the earliest convolutional nns. It was built to read small images of handwritten numbers (MNIST dataset) and correctly classify which digit was represented in the image. \n",
    "\n",
    "how it works:\n",
    "- Layer C1 is a convolutional layer. It scans the input image for features it learned during training. It outputs a map of where it saw each of its learned features in the image. the \"activation map\" is downsampled in layer S2. \n",
    "\n",
    "- Layer C3 is another convolutional layer. This layer scans C1's activation map for combinations of features. It also puts out an activation map describing the spatial locations of these feature combinations which is downsampled in layer S4. \n",
    "\n",
    "- Finally the fully-connected layers at the end, F5, F6, and OUTPUT are a classifier that takes the final activation map and classifies it into one of ten bins representing 10 digits.\n",
    "\n",
    "In code the nn is represented by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e66950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution (kernel size) \n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) # one input channel (grayscale image), 6 output channels (feature maps), kernel size (3x3) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine ooperation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6 * 6 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2) window \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # if the size is a square you can only specify a single number \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # flattens the image \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension \n",
    "        num_features = 1\n",
    "        for s in size: \n",
    "            num_features *= s \n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcb1b3",
   "metadata": {},
   "source": [
    "The above code shows a typical PyTorch Model: \n",
    "- inherits from ```torch.nn.Module``` - modules may be nested, in fact, even the ```Conv2d``` and ```Linear``` classes inherit for ```torch.nn.Module``` \n",
    "\n",
    "- A model will have a ```__init__()``` funtion where it instantiates its layers, and loads any data artifacts it might need (e.g. an NLP model might load a vocabulary)\n",
    "\n",
    "- A model will have a ```forward()``` function. This is the actual computation happens. An input is passes through the network layers and various functions to generate output. \n",
    "\n",
    "- Aside from these facts we can build our model like any other Python class adding whatever properties and methods needed to support model computation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cc759",
   "metadata": {},
   "source": [
    "understanding the code \n",
    "## structure \n",
    "```__init__()``` \n",
    "gathers the tools needed to start \n",
    "\n",
    "```conv1, conv2``` \n",
    "concolutional layers \"feature detectors\" such as edges curves, corners. \n",
    "```conv1``` - takes one input channel and creates 6 \"feature maps\" using a 3x3 kernel. \n",
    "\n",
    "```self.fc*``` \n",
    "\"Fully connected\" (linear) layers. Once convolution layers have found the features these layers act as a traditional brain to make sense of them and decide: \"Based on these curves, this is likely the number 5.\"\n",
    "\n",
    "## Data Flow \n",
    "```forward()``` method defines the path the image takes through the network. \n",
    "\n",
    "step 1. <strong>Convolution</strong>: scans the image for patterns \n",
    "\n",
    "step 2. <strong>ReLU</strong>: Activation function that turns negative values to zero. (add non-linearity)\n",
    "\n",
    "step 3. <strong>Max Pooling</strong>: shrinks the image size by half to reduce computation and focus on the most important features. \n",
    "\n",
    "step 4, <strong>Flattening</strong>: Converts the 3D cube of the data into a 1D long list of numbers, so the \"linear\" layers can read it. \n",
    "\n",
    "step 5. <strong>Output</strong>: The final layer ```fc3``` produces 10 numbers. The highest number represents the networks \"guess\"\n",
    "\n",
    "## Flattening Math \n",
    "```self.fc1 = nn.Linear(16 * 6 * 6, 120)```\n",
    "The network expects the input to be flattened to a single vector of 576 elements. (16 * 6 * 6, 120)\n",
    "- 16 is the number of channels from the previous layer. \n",
    "\n",
    "- 6 * 6 is the height and width of the data after it has been shrunk down by pooling layers. \n",
    "\n",
    "```num_flat_features``` calculates this total (16 * 6 * 6 = 576) automatically. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d984b03",
   "metadata": {},
   "source": [
    "## fc Layers \n",
    "```\n",
    "self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6 * 6 from image dimension \n",
    "self.fc2 = nn.Linear(120, 84)\n",
    "self.fc3 = nn.Linear(84, 10)\n",
    "```\n",
    "This is a decision making funnel. Each layer condenses the information further to reach the final answer.\n",
    "\n",
    "fc1 - input size: 576, output size: 120; Takes all the detected patterns (edges, \n",
    "                                         circles) and starts combining them into \"parts\" of a number.\n",
    "\n",
    "fc2 - input size: 120, output size: 84;  Further compresses these parts into \n",
    "                                         abstract concepts.\n",
    "\n",
    "fc3 - input size: 84, output size: 10;   The final \"vote.\" Each of the 10 \n",
    "                                         outputs represents a digit (0â€“9). The highest value is the winner.\n",
    "\n",
    "### Function \n",
    "The operation happening inside these layers is the affine transformation\n",
    "$$y = xA^T + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04c53c",
   "metadata": {},
   "source": [
    "## Forward Pass \n",
    "\"Engine room\" of the model. \n",
    "```x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))``` performs convlution, ReLU activation, and max pooling all in once line. \n",
    "\n",
    "```conv1(x)``` - looks for features\n",
    "\n",
    "```F.relu(...)``` - Rectified Linear Unit, If a value is negative, turn it to 0. If it's positive, keep it.\n",
    "\n",
    "```F.max_pool2d(...,(2,2))``` - Downsampling looking at 2x2 squares and keeps the largest value (throwing the other 3 away)\n",
    "\n",
    "### Flattening the Data \n",
    "Before the data can enter he Linear Layers it has to be changed from 3D to 1D list. \n",
    "\n",
    "```x = x.view(-1, self.num_flat_features(x))``` \n",
    "\n",
    "```x.view()``` - PyTorch way of resizing a shape. \n",
    "\n",
    "```-1``` - Wildcard telling PyTorch \"I don't know how many images are in the batch, so just figure that part out automatically.\"\n",
    "\n",
    "The -1 is one of the most useful tricks in PyTorch. It stands for \"everything else.\"\n",
    "\n",
    "When you run x.view(-1, 784), you are telling PyTorch to rearrange those same 7,840 numbers into a 2D table (a matrix).\n",
    "- **The 784 part**: This defines the Columns. You are saying \"I want every image to be represented by a single horizontal line of 784 pixels.\n",
    "- **\"The -1 part**: This is an \"Auto-Calculate\" button for the Rows. PyTorch looks at the total numbers (7,840) and says: \"If I need 784 columns, I must need 10 rows to fit all the data.\" ($7840 / 784 = 10$).\n",
    "\n",
    "The New Shape: (10, 784)\n",
    "\n",
    "```self.num_flat_features(x)``` - calculates total number of pixels. \n",
    "\n",
    "### The Fully Connected Layers \n",
    "Now that the data is a flat list of features it passes through the brain of the network. \n",
    "\n",
    "```\n",
    "x = F.relu(self.fc1(x))\n",
    "x = F.relu(self.fc2(x))\n",
    "x = self.fc3(x)\n",
    "```\n",
    "\n",
    "Will run through the first decision layer and apply activation function to keep important features. It will do the same for the second decision layer and finally the final decision to return. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05798ce",
   "metadata": {},
   "source": [
    "## Calculator (num_flat_features)\n",
    "figures out exactly how many numbers are in a single image after it has passed through the convolutional layers.\n",
    "\n",
    "```size = x.size()[1:]``` \n",
    "\n",
    "```x.size()``` will return the size of the tensor x in form of (batch_size, channels, height, width)\n",
    "\n",
    "```[1:]``` asks for everything except the first number of the list (size becomes [16, 6, 6])\n",
    "\n",
    "```\n",
    "num_features = 1\n",
    "for s in size: \n",
    "    num_features *= s \n",
    "return num_features\n",
    "```\n",
    "will calculate the number of features for each image in the batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe98e0",
   "metadata": {},
   "source": [
    "Lets instatiate this object and run a sample input through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bed3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[ 0.0524,  0.0014,  0.0689, -0.0758,  0.0945, -0.0307, -0.1527, -0.0552,\n",
      "          0.0888,  0.0073]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net) # what does it tell us about itself \n",
    "\n",
    "input = torch.rand(1,1, 32, 32) # first parameter represents the batch_size the rest are (1 color channel, 32 height, 32 width)\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input) #forward is not called directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae4f80",
   "metadata": {},
   "source": [
    "A subclass ```torch.nn.Module``` will reort the layers it created and their shapes & parameters. Provides an overview of a model if we want to get the gist of its processing. \n",
    "\n",
    "```input = torch.rand(1, 1, 32, 32)``` creates a dummy input representing a 32x32 image with 1 color channel. Usually this is an image tile loaded in and converted to a tensor of this shape. \n",
    "\n",
    "(1 batch_size, 1 color channel, 32 height, 32 width)\n",
    "\n",
    "Output of net(input) is the models confidence that the input is a particular digit. (as of now since the model hasn't learned abnything yet lets not expect any signal in the output). \n",
    "\n",
    "The shape of the output batch dimension should match the input batch dimension. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
