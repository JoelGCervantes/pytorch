# Intro to Autograd 

## basic mechanics of a single training pass 
example using a basic RNN
begin with 4 tensors: x the input, h hidden state of the rnn that gives it its memory, and 2 sets of learning weights; one each for the input and the hidden state 

```
x = torch.randn(1,10)
prev_h = torch.randn(1,20)
W_h = torch.randn(202,20)
W_x = torch.randn(20,10)

# multiply weights by respective tensors
i2h = torch.mm(W_x, x.t())
h2h = torch.mm(W_h, prev_h.t())

# add the outputs of the two matrix multiplications
# pass result through an activation function 
next_h = i2h + h2h 
next_h = next_h.tanh() # hyperbolic tan is the activation function here 

# compute loss 
loss = next_h.sum() # difference between the correct output and the actual prediction of the model.

# We have:
# taken a training input
# run it through a model
# gotten an oputput and determined the loss
```

At this point in the training loop we have to compute the derivatives of that loss with respect to every parameter of the model

We then have to use the gradients over the learning weights to decide how to adjust those weights in a way that reduces the loss.  

```
# this can be achieved in one line of code 
loss.backward()

# Each tensor generated by this computation knows how it came to be by carrying metadata indicating where it came from
# i2h metadata says it came from mm of W_x and x
# from this data the bacward method rapidly calculates the gradients the model needs for learning. 
```