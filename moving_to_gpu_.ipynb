{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t3Ib0QYyXQxe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trT5kdnSXONf",
        "outputId": "47fddbba-1c9d-4622-dd18-622cb3a22bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('On GPU')\n",
        "else:\n",
        "    print('Sorry, CPU only')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIZen9R2Xbd6"
      },
      "source": [
        "after determining we have a GPU(s) available, we need to put our data where the GPU can see it. CPU performs computations on computers RAM. GPU has dedicated memory attached to it. WHenever you want to perform a cmputattion on a device, you must move *all* the data needed for that computation to memory accessible by that device. (moving the data to memory accessible by the GPU = moving the data to the GPU)\n",
        "\n",
        "Multiple ways to move data to the GPU. At creation time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCtEI9aUXTZX",
        "outputId": "be18a5fc-3e75-4607-9bf7-327fdf7157a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.3340, 0.2380],\n",
            "        [0.1306, 0.9463]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  gpu_rand = torch.rand(2, 2, device='cuda')\n",
        "  print(gpu_rand)\n",
        "else:\n",
        "  print('no gpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0huuqPKYsm7"
      },
      "source": [
        "By default, new tensors are created on the CPU, so we have to specify when we want to create our tensor on the GPU with the optional ```device``` argument. Pytorch informs us which device it's on (if not on CPU)\n",
        "\n",
        "We can query th number of GPU's with ```torch.cuda.dvice_count()```. If we have more than one GPU, we can specify them by index: ```device='cuda:0'```, ```device='cuda:1'```, etc\n",
        "\n",
        "Specifying devices everywhere with string constants is fragile. We create a device handle that can be passed to our tensors instead of a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZIqDMjYYdoP",
        "outputId": "fc4334b3-5ec3-43e7-cdb8-315191bacb82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "tensor([[0.8352, 0.8573],\n",
            "        [0.9296, 0.2123]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  my_device = torch.device('cuda')\n",
        "else:\n",
        "  my_device = torch.device('cpu')\n",
        "print('device: {}'.format(my_device))\n",
        "\n",
        "x = torch.rand(2, 2, device=my_device)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXS-akvkaNX6"
      },
      "source": [
        "If there is an existing tensor living on one device, you can move it to another with the ```to()``` method. The following line of code created a tensor on CPU, and moves it to whichever device handle you acquired in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Laas3z4raKyl",
        "outputId": "d02af87d-e3b3-4a30-8fd2-2d6a55119740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5743, 0.4909],\n",
            "        [0.9951, 0.1492]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "y = torch.rand(2, 2)\n",
        "y = y.to(my_device)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXZOqKH7a1IE"
      },
      "source": [
        "In a computation involving two or more tensors, *all the tensors must be on the same device*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "VdjHGHm1atri",
        "outputId": "1846a995-3bd2-4787-96bd-4efe050fc4e6"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-984435977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m   \u001b[0;31m#except will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 2,)\n",
        "y = torch.rand(2, 2, device='gpu')\n",
        "z = x + y   #except will be thrown"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
